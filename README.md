# ラビットチャレンジ 深層学習day1  
  

## Section1:入力層～中間層  
  
要点   
・ニューラルネットワークは、入力層、中間層、出力層からなる。  
・入力された変数は、中間層で変換され、出力層に渡される。  
・中間層では、線形変換に加えて活性化関数を通すことで、非線形な関数も表現する事ができる。  
  
実装演習（1_1_forward_propagation.ipynb）  
・パラメータの初期値やノード数を変更した。  
・層ごとの重みが行列で表される事が理解できた。  
  
確認テスト  
・ニューラルネットは線形結合と活性化関数の繰り返しで書ける。  
・行列とベクトルの積は、numpyのnp.dotで書ける。  
  
## Section2:活性化関数  
  
要点  
・活性化関数という非線形の変換を加えることで、  
ニューラルネットワークの表現力を上げる事ができる。  
・ステップ関数、シグモイド関数、ReLU関数などがある。  
・ReLU関数が勾配消失の問題を解決し、広く活用されている。  
  
実装演習（1_1_forward_propagation.ipynb）  
・実装ではrelu関数を使用している。  
・中間層と出力層では活性化関数の役割が違う為、関数の種類も異なる。  
  
確認テスト  
・加法性と斉次性を満たす写像を線形写像と言う。  
・ソースコード上で、容易に活性化関数の種類を変える事ができる。  
  
## Section3:出力層  
  
要点  
・出力層は実際の予測を出力する。  
・回帰は恒等関数、二値分類はシグモイド関数、多クラス分類はソフトマックス関数が用いられる。  
・予測と正解を誤差関数に入力する事で誤差を得る。  
・回帰は二乗誤差、分類はクロスエントロピー誤差が用いられる。  
  
実装演習（1_1_forward_propagation.ipynb / 1_2_back_propagation.ipynb）  
・ライブラリーを使わずに、偏微分されたパラメータの勾配の計算を実装した。  
・学習率を変えてパラメーターの変化を観測した。  
  
確認テスト  
・二乗誤差の前につく1/2は、本質的な意味はない。  
・ソフトマックス関数では、値を確率化するために、全てのクラスの確率の和で割っている。  
  
## Section4:勾配降下法  
  
要点  
・パラメーターの最適化に勾配降下法が用いられる。  
・学習率が大きすぎると発散し、小さすぎると収束が遅くなる。  
・ランダムにサンプルしたデータでの勾配法を確率的勾配法と言う。  
・通常はランダムに分割したデータ集合で計算するミニバッチ勾配降下法を用いる。  
    
実装演習（1_3_stochastic_gradient_descent.ipynb）  
・活性化関数のシグモイド関数への変更を試した。  
・学習率を適切に下げていく事で誤差が収束していく様子が見えた。  
  
確認テスト  
・新しく取得できるデータを逐次的に学習する事をオンライン学習と言う。  
・微分した値に学習率をかけた値がパラメータの更新量となる。  
  
## Section5:誤差逆伝播法  
  
要点  
・誤差の計算では、微分の連鎖律の性質を用いて効率的に計算できる。  
・ノードを逆にたどっていく事で不要な再帰的計算を避ける事が可能。  
・通常の数値微分より効率的。  
  
実装演習（1_4_1_mnist_sample.ipynb）  
・MINISTの分類で92%の精度を得た。  
・中間層の数や繰り返しの回数を変更する事で更に精度が向上した。  
  
確認テスト   
・シグモイド関数等の関数の導関数をあらかじめ用意しておき、誤差逆伝播法でそれを利用する。  
・実務では、tensorflowやpytorch等のライブラリが計算してくれる。  


